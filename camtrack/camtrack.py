#! /usr/bin/env python3

__all__ = [
    'track_and_calc_colors'
]

from typing import List, Optional, Tuple

import numpy as np
import cv2

import tqdm
from corners import CornerStorage
from data3d import CameraParameters, PointCloud, Pose
import frameseq
from _camtrack import (
    PointCloudBuilder,
    create_cli,
    calc_point_cloud_colors,
    pose_to_view_mat3x4,
    to_opencv_camera_mat3x3,
    view_mat3x4_to_pose,
    rodrigues_and_translation_to_view_mat3x4,
    TriangulationParameters,
    build_correspondences,
    triangulate_correspondences,
    eye3x4,
    view_mat3x4_to_rodrigues_and_translation
)
from ba import run_bundle_adjustment


def retriangulate_points(proj_mat, view_mat_sequence, points2d_sequence):
    """
    это функция для ретриангулирования (то есть восстановления положения (координат) в пространстве) одной 3d-точки
    по набору кадров, на каждом из которых встречается проекция этой 3d-точки (эти проекции со всех
    кадров образуют последовательность 2d-точек (проекция 3d-точки - это 2d-точка), которую подают на вход этой
    функции в переменную points2d_sequence); также на каждом кадре должно быть известно положение камеры - все
    эти положения со всех кадров подаются в эту функцию в виде послежовательности view-матрицы камеры в переменную
    view_mat_sequence; также в функцию подаётся матрица проекции proj_mat

    функция возвращает координаты 3d-точки (которая проецируется в точки из points2d_sequence)
    """
    assert (len(view_mat_sequence) == len(points2d_sequence))

    Pi_sequence = [proj_mat @ view_mat for view_mat in view_mat_sequence]
    points2d_homog = [np.hstack((point2d, 1)) for point2d in points2d_sequence]

    num = len(Pi_sequence)
    M = np.zeros((3 * num, 4 + num))
    for i, (x, Pi) in enumerate(zip(points2d_homog, Pi_sequence)):  # вся функция - по аналогиии с практикой 6
        M[3 * i:3 * i + 3, :4] = Pi
        M[3 * i:3 * i + 3, 4 + i] = -x
    V = np.linalg.svd(M)[-1]
    X = V[-1, :4]
    return (X / X[3])[:3]


def choose_best_next_frame_and_solve_pnp(left_lim_1, right_lim_1, left_lim_2, right_lim_2,
                                         found_3d_points, corners_id_for_3d_points,
                                         intrinsic_mat, corner_storage, PNP_ERROR, MIN_INLIERS,
                                         frame_with_found_cam, view_mats):
    """
    Эта функция выбирает кадр, для которого следующим искать положение камеры в нем.

    Для этого мы будем кадр только среди кадров, соседних с теми, для которых уже нашли позицию камеры (ведь так больше
    шансов найти кадр с наибольшим количество 2d-3d соответствий) - как мы уже говорили,
    у нас две области номеров таких кадров: [left_lim_1 ... right_lim_1] и [left_lim_2 ... right_lim_2] -
    - так что, соседние с ними кадры (их, очевидно, не больше четырёх) и перебираем.

    А далее для каждого такого кадра решаем задачу pnp - и как только она решится, мы считаем, что нашли подходящий
    кадр - его и выдаем, а заодно границы областей, для которых нашли позицию камеры, а также возвращаем результат
    pnp - параметры позиции камеры и число инлайеров - точек, по которым посчитана pnp

    Изначально мы пытаемся решить pnp с маленькой ошибкой, но если ни на одном из рассматриваемых кадрах это не
    получается сделать, то увеличиваем её...
    """
    interesting_frames = []  # номера кадров, которые можно сейчас рассмотреть - это соседние кадры с кадрами, для
    # которых уже известны положения камеры (так больше шансов найти 2d-2в соответствия)

    if left_lim_1 > 0:
        interesting_frames.append((left_lim_1 - 1, left_lim_1))
    if right_lim_2 < len(corner_storage) - 1:
        interesting_frames.append((right_lim_2 + 1, right_lim_2))
    if right_lim_1 < left_lim_2 - 2:
        interesting_frames.append((right_lim_1 + 1, right_lim_1))
        interesting_frames.append((left_lim_2 - 1, left_lim_2))
    if right_lim_1 == left_lim_2 - 2:
        interesting_frames.append((right_lim_1 + 1, right_lim_1))

    # то есть ещё раз: у нас на данный момент известны положения камеры в кадрах с номерами, которые лежат в такой
    # области (состоит из двух непрерывных кусков): [left_lim_1, right_lim_1] и [left_lim_2, right_lim_2] -
    # как мы уже говорили, следующим кадром, на котором мы будем искать положение камеры, будет один из кадров,
    # который является соседним (по своему номеру) с этой областью... таких кадров не более четырёх (их номера:
    # left_lim_1 - 1, right_lim_1 + 1, left_lim_2 - 1, right_lim_2 + 1), но не все из них могут подходить (так как
    # среди этих четырёх кадров могут быть два одинаковых или некотррые из них могут не существовать (если номер < 0
    # или > len(corner_storage) - 1 (так как у на всего кадров ровно len(corner_storage) - 1, нумерация с нуля))).

    # на данный момент interesting_frames содержит в себе номера всех кадров, которые нам подходят (как уже сказали их
    # более четырёх) - даже более того, interesting_frames содержит пары из номера кадра, который нам подходит (то есть
    # кадра, соседнего с областью кадров с известными положениями камеры) и номера кадра, соседнего с этим кадром и
    # находящимся в области кадров с известными положениями камеры - например, interesting_frames может содержать
    # такую пару: (left_lim_1 - 1, left_lim_1) - то есть первый элемент пары - это номер кадра, на котором мы хотим
    # найти положение камеры, а второй элемент пары - это номер соседнего кадра, для которого уже знаем положение камеры

    # теперь нам остаётся лишь перебрать кадры из interesting_frames, на каждом попытаться найти положение камеры и
    # в качестве результата выбрать тот кадр, на котором это положение лучше всегь нашлось

    interesting_frames = sorted(interesting_frames, key=lambda pair: pair[0])  # чтобы удобнее перебирать кадры,
    # сортируем - мы будем искать положение камеры по порядку для каждого кандидата из interesting_frames

    """ещё один момент - если сейчас область кадров с известными положениями камеры у нас состоит из двух кусков,
    между которыми есть ещё кадры, для которых положение камеры неизвестно (это значит, что right_lim_1 < left_lim_2 - 1
    -> тогда для кадров с номерами right_lim_1+1, ...,left_lim_2 - 1 положения камеры ещё неизвестны), то один из этих
    кадров (а именно right_lim_1+1 - он находится среди interesting_frames, тк является соседом нашей области)
    в interesting_frames мы поставим на первое место... это чисто интуитивное соображение: мы хотим сначала найти 
    положения камеры на одной непрерывной области вида [left_lim_1, right_lim_2], а затем уже донаходить положения
    камеры на сосдених кадрах слева и справа от это области - интуитивно так более точно получится, ведь если мы знаем
    положения камеры на непрерывном куске кадров, то и триангуляция/ретриангуляция потом может быть поточне..."""
    if (left_lim_1 > 0) and (right_lim_1 < left_lim_2 - 2):
        if left_lim_1 > (left_lim_2 - right_lim_1):
            interesting_frames[0], interesting_frames[1] = interesting_frames[1], interesting_frames[0]
    """-----"""

    print("Текущее облако точек имеет размер = ", len(corners_id_for_3d_points), ",")
    print("Рассматриваем кадры с номерами: ", [pair[0] for pair in interesting_frames], ",")

    """
    Итак, делать будем так. Мы будем по порядку проходить по кадрам в interesting_frames и на каждом будем пытаться
    решить задачу pnp (-это задача и есть нахождение положения камеры на кадре). Как только для какого-то кадра это
    удалось, мы сразу прекращаем рассматривать остальные кадры и в качестве результата выдаём найденное положение камеры
    на этом кадре. Если же мы перебрали все кадры в interesting_frames, но ни для какого кадра рещить pnp не удалось, то
    мы увеличиваем допустимую ошибку решения задачи pnp (домножаем её на определнный коэффициент, который увеличивается)
    и начинаем сначала: по порядку перебираем кадры из interesting_frames и решаем задачу pnp... 
    Изначально мы начинаем с маленькой ошибки pnp и дальше она наращивается по мере необходимости...
    
    Поэтому как только мы нашли кадр, на котором задача pnp решилась, мы сразу считаем это за ответ,
    ведь чем раньше такой кадр найдётся, тем меньше ошибка pnp используется в решнии, а значит тем лучше на этом
    кадре удалось получить положение камеры. Конечно, у нас ошибка pnp растёт только после полного прохождения всего
    interesting_frames, а потому может быть так, что кроме первого кадра, на котором решилась pnp, эта задача с той же
    самой ошибкой pnp решится и на других кадрах interesting_frames, которые идут после этого кадра... но мы их не 
    рассматриваем, ведь мы не зря перед этим сортировали кадры в interesting_frames (а потом выводили кадр right_lim_1+1
    на первое место) - порядок кадров в interesting_frames отражает порядок наших предпочтений, на каком кадре мы хотим
    найти положение камеры. Поэтому как только задача pnp решилась, то для нас это и будет ответ, так как с одной 
    стороны, эта задача решилась для наименьшей обшибки pnp, а с другой стороны - это первый кадр в нашх
    предпочтениях (так как мы перебраем кадры в interesting_frames по порядку - как раз по нашим предпочтнениям).
    
    В чём тут есть сложность? Изначально (до увеличения) ошибка решение pnp у нас равна PNP_ERROR. И сложность в том,
    чтобы её выбрать. Если взять слишком большую, то будет неточно решаться сама задача pnp, а если взять маленькую,
    то при решении задачи pnp будет много аутлайеров (то есть тех точек, которые не соответствуют решению задачи pnp
    с данной ошибкой) среди двумерных соответствий - а все аутлайеры мы выкидываем из облака 3d-точек... поэтому при 
    маленькой ошибке репроекции могут очень быстро закончиться точки в облаке 3d-точек... У меня лучшим результатом было
    PNP_ERROR где-то 2.5-3.    
    """
    best_frame = -1  # лучший кадр, который и ищем (изначально не знаем - ставим -1) среди кандидатов в interesting_frames
    res = False  # пока что pnp не решили
    break_from_while = False  # пока из while выходить не нужно
    curr_coeff_er = 1  # именно на этот коэффициент домножаем ошибку решения PNP (он будет увеличиваться)
    while True:
        for intr_frame, neighbor_to_intr_frame in interesting_frames:
            corners_in_frame = corner_storage[intr_frame]  # взяли уголки с выбранного кадра

            print("Обрабатываем кадр номер", intr_frame, "...")
            print("                 попытка решить pnp c ошибкой (в пикселях) = ", PNP_ERROR * curr_coeff_er)

            # в следующих 4 строчках получаем 3d и 2d точки, соответствующие друг другу:

            common_frame_and_3d = np.intersect1d(corners_id_for_3d_points, corners_in_frame.ids)  # получаем те id,
            # которые встречаются и среди 3d-точек и среди уголков на кадре
            mask_common_ids_3d_in_frame = np.in1d(corners_id_for_3d_points, corners_in_frame.ids)
            mask_common_ids_frame_in_3d = np.in1d(corners_in_frame.ids, corners_id_for_3d_points)
            points3d_for_frame = found_3d_points[mask_common_ids_3d_in_frame]  # 3d-точки
            points2d_for_frame = corners_in_frame.points[mask_common_ids_frame_in_3d]  # 2d-точки (уголки) на кадре

            # в результате points3d_for_frame и points2d_for_frame - это 3d-2d соответствия - то есть points3d_for_frame
            # - это те 3d-точки из имеющегося у нас облака 3d-точек (положения которых мы уже знаем), проекции которых
            # присутствуют на рассматриваемом нами кадре intr_frame - эти проекции
            # являются уголками и собраны в points2d_for_frame

            if not len(points2d_for_frame) >= 4:  # если не нашли хотя бы 4 точки, то pnp точно не решить - идем дальше
                continue
            assert (len(points2d_for_frame) == len(points3d_for_frame))

            """
            Итак, мы рассматриваем кадр с номером intr_frame - хотим найти на нём положение камеры. Для этого нам нужно
            решить задачу pnp - это мы уже можем сделать, так как у нас есть 3d-2d соответствия между облаком 3d-точек
            и уголками этого кадра. Но, чтобы решение задачи pnp было более точным, мы хотим использовать некоторое
            начальное приближение для решения задачи pnp - то есть хотим знать приближнное положение камеры на кадре
            intr_frame. Есди у нас будет это приближенное положение, то дальше, используя 3d-2d соответствия, мы
            довольно быстро уточним это положение и получим ответ-положение камеры на этом кадре (наличие приближённого
            положения, во-первых, уточняет итоговое решение (так как нам есть с чего стартовать при решении), 
            а во-вторых, защищает от больших ошибок в определении положеня камеры, так как в процессе решения задачи
            pnp иногда может случиться такое, что в качестве решения задачи pnp будет найден "локальный минимум", 
            который будет далёк от истинного решения...)).
            
            Но как же найти это приближённое положение камеры на кадре intr_frame? Мы помимо кадра intr_frame имеем
            ещё и его соседа, для которого положение камеры уже известно (эти соседи мы вместе с интересующими кадрами
            храним в interesting_frames) - а именно, кадр с номером neighbor_to_intr_frame. Очевидно, что на соседних
            кадрах (соседних по номеру - то есть в последовательности кадров (в видео, например) эти кадры идут один
            за другим) положения камеры близки - поэтому за приближённое положение камеры на интересующем кадре
            intr_frame мы возмём уже известное положение камеры на neighbor_to_intr_frame.
            """
            neighbor_view = view_mats[frame_with_found_cam.index(neighbor_to_intr_frame)]  # view-матрица камеры
            # на neighbor_to_intr_frame - это фактически и есть положение камеры.
            neighbor_rvec, neighbor_tvec = view_mat3x4_to_rodrigues_and_translation(neighbor_view)  # представим это
            # положение камеры в виде матрицы поворота neighbor_rvec и вектора смещения neighbor_tvec

            # наконец решаем задачу pnp на кадре intr_frame:
            res, rvec, tvec, inliers = cv2.solvePnPRansac(objectPoints=points3d_for_frame,  # 3d-2d соответствия
                                                          imagePoints=points2d_for_frame,
                                                          cameraMatrix=intrinsic_mat,  # матрица внутренних параметров камеры
                                                          reprojectionError=PNP_ERROR * curr_coeff_er,  # текущая ошибка решения задачи pnp
                                                          distCoeffs=None,
                                                          iterationsCount=3000,
                                                          useExtrinsicGuess=True,
                                                          rvec=neighbor_rvec.copy(),  # начальные значения положения камеры, чтобы проще решить pnp
                                                          tvec=neighbor_tvec.copy())  # (важно скопировать (.copy())!
            tvec = tvec.reshape(-1, 1)

            # итак, после решения pnp мы получили:
            # res - результат (True если успешно удалось решить задачу pnp с данной ошибкой reprojectionError и
            # False иначе), rvec и tvec - положение камеры на intr_frame, inliers - список id тех 3d-2d соответствий,
            # которые являются инлайерами в данном решении

            if inliers is not None:
                current_outliers = np.delete(common_frame_and_3d, inliers.flatten())  # удаляем из всех соответствий
                # инлайеры -> остаются аутлайеры для текущего решения pnp

            if res:  # если решили задачу pnp:
                best_frame = intr_frame  # фиксируем кадр
                if (len(inliers) >= MIN_INLIERS) or (PNP_ERROR * curr_coeff_er > 5):  # если достаточно инлаеров,
                    # по которым решили pnp или ошибка уже достаточно большая, что нам уже не важно, сколько
                    # инлаеров - решить бы хоть как-то, то просто выходим из всех циклов и выдаем затем результат
                    break_from_while = True    # (если же нет, то крутимся в циклах, пытаясь решить далее)
                    break

        if break_from_while: break

        if PNP_ERROR * curr_coeff_er > 20:  # если ошибка уже очень большая, то выходим - что-то не так с видео
            break

        curr_coeff_er += 0.5  # если ранее не вышли из цикла, то так и не решили задачу... - продолжаем с большим коэфф
        print("ВНИМАНИЕ: pnp на базовой ошибке репроекции = ", PNP_ERROR,
              " не решилась ни для какого кадра...")
        print("          Увеличили ошибку репроекции в ", curr_coeff_er, "раз и повторили попытку...")

    if not res:  # если из while вышли, но не решили pnp - выводим ошибку
        raise NameError("Не удалось решить pnp - вдимо, странное видео")

    # теперь мы нашли положение камеры и на кадре intr_frame -> область кадров с известными положениями камеры
    # увеличилась -> нужно обновить её:
    new_left_lim_1, new_right_lim_1, new_left_lim_2, new_right_lim_2 =\
        left_lim_1, right_lim_1, left_lim_2, right_lim_2
    if best_frame == left_lim_1 - 1:  # двигаем нужную границу найденных позиций камеры
        new_left_lim_1 = best_frame
    elif best_frame == right_lim_1 + 1:
        new_right_lim_1 = best_frame
    elif best_frame == left_lim_2 - 1 and right_lim_1 < left_lim_2 - 2:
        new_left_lim_2 = best_frame
    elif best_frame == right_lim_2 + 1:
        new_right_lim_2 = best_frame

    return best_frame, new_left_lim_1, new_right_lim_1, new_left_lim_2, new_right_lim_2, \
           rvec, tvec, inliers, current_outliers  # результат функции


def get_initial_frames(corner_storage, intrinsic_mat):
    """
    Данная функция получает на вход последовательность уголков corner_storage и матрицу внутренних параметров камеры
    intrisic_mat. Далее эта функция должна на двух каких-то кадрах последовательности получить положения камеры
    (найти view-матрицы камеры на этих кадрах) - и вернуть их вместе с номерами кадров. Эта процедура называется
    инициализацией и использует сложную эпиполярную геометрию (см лекции) - но всё это скрыто внутри функций из cv2.

    Сразу заметим, что вся задача трекинга (то есть поиска положений камеры на каждом кадре последовательности)
    решается с точностью до масштаба и точки отсчёта (ведь по одним только фотографиям мы никак не можем определить
    масштаб, а точка отсчёта (начальная точка, начало координат) - это вообще весьма условное понятие, которое вводится
    лишь для вычислений...) - это значит, что те положения камеры, которые мы ищем (а каждое положение камеры
    задаётся двумя параметрами: матрицей поворота и вектором переноса (вектор переноса ищется относительно некоторой
    начальной точки, а матрица поворота ищется относительно некоторого направления)), ищутся с точностью до масштаба
    (это значит, что все векторы переноса можно умножать на любое ненулевое число (разумеется все векторы переноса
    всех положений камеры должны одновременно умножаться на одно и то же число) (это и есть изменение масштаба), но это
    будет считаться тем же результатом трекинга (то есть два положения камеры, отлючающиеся только домножением вектора
    переноса на константу - это одно и то же положение)) и с точностью до точки отсчёта (и в частности - с точностью до
    начального направления, относительно которого считаем поворот камеры) (это значит все векторы переноса мы можем
    сдвигать на один и тот же вектор (это является сдвигом точки отсчёта), а все матрицы поворота мы можем поворачивать
    на один и тот же угол (это изменение начального направления) - и это не меняет результат трекинга). Ну и это
    логично, так как главная задача трекинга - найти не абсолютные положения камеры в пространстве (для чего нужно
    вводить точку отсчёт и тд), а найти относительные положения камеры в кадрах (то есть положения камеры в одном
    кадре относительно положения в другом кадре) - а для них масштаб или точка отсчёт неважны -> поэтому мы можем брать
    любую точку отсчёта.

    Так как во время инициализации нам необходимо найти положения камеры в двух кадрах (обозначим их номера как
    first_frame и second_frame - какие именно это кадры, мы решим в процессе инициализации, будем брать те, которые
    удобнее), то удобно будет положение камеры в first_frame принять за точку отсчёта (а направление камеры в этом кадре
    - за начальное направление) - как уже сказали можем любую точку отсчёт выбрать. В этом случае задача трекинга
    сводится к поиску положений камеры относительно положения в first_frame.
    То есть положение камеры в second_frame - это просто положение камеры отсноительно first_frame. А положение
    камеры в first_frame задаётся единичной матрицей поворота (такая матрица соответствует отсутствию вращения)
    и нулевым вектором переноса (такой вектор соответствует отсутствию смещения) - просто потому что положение камеры
    в first_frame у нас считается начальным.

    Короче, для инициализации нам нужно выбрать два кадра first_frame и second_frame после чего нужно найти положение
    камеры в second_frame относительно положения в first_frame. Как всё это делается - обсуждалось в лекции...
    """
    print("Начинаем инициализацию (поиск положений камеры на двух кадрах)...")

    best_frames = None  # пара (в виде tuple) номеров кадров, для которых найдем положения камеры - изначально None
    best_R_t = None  # положение камеры во втором из этих кадров относительно первого (в виде tuple из R -
    #                                                                 матрица поворота и t - вектор переноса)
    best_inliers = 0
    frames_all = len(corner_storage)  # количество всех кадров

    """В инициализации нам нужно найти два кадра, для которых будем искать положения камер - для этоого мы просто
    переберём пары кадров и для каждой пары попытаемся найти положения камеры на этих кадрах (точнее, как уже говорили,
    будем исктаь относительное положение камеры на втором кадре). Искать будем как рассказывалось в лекции. А в качестве
    итоговых двух кадров будем брать ту пару, для которой лучше всего (с наибольшим значением best_inliers) посчиталось
    положение камеры.
    
    Чтобы работало быстрее, будем в качестве первого кадра (номер i) перебирать кадры в первой трети всех кадров,
    а в качестве второго кадра (j) - все кадры после i. Перебирать будем с шагом 5 опять же для скорости:"""
    for i in tqdm.tqdm(range(0, frames_all // 3, 5)):
        for j in range(i + 5, frames_all, 5):
            correspondences = build_correspondences(corner_storage[i], corner_storage[j])  # двумерные соответствия
            # для кадров i и j (то есть получаем уголки, которые встречаются на обоих кадрах)
            # (в частности, correspondences.points_1 - это координаты этих уголков на первом кадре (кадре i),
            # а correspondences.points_2 - координаты этих же уголков (с теми же id), но на втором кадре (j))

            if len(correspondences.ids) < 200:  # если мало соответствий, идём дальше (тк положение, полученное по
                # малому колтичеству соответствий будет неточным)
                continue  # число 200 - чисто эмпирическая константа!!! возможно, можно другую получше взять...

            """
            далее по двумерным соответствиям ищем матрицу гомографии (homography_mat) и существенную матрицу
            (essential_mat); для каждой из матриц в процессе их вычисления считается маска (mask_homography и 
            mask_essential соответственно для матрицы гомографии и существенной матрицы), в которой для каждого 
            двумерного соответствия (то есть для каждого уголка, который встречается на обоих кадрах i и j) стоит 1, 
            если это двумерное соответствие является инлайером при вычислении соответствующей матрицы (то есть если
            по этому двумерному соответствию считалась матрица, то есть это двумерное соответствие корректно для
            данной матрицы (например, для матрицы гомографии инлайером является то двумерное соответствие, которое
            удовлетворяет гомографии, задающейся этой матрицей)) и 0, если наоборот - это двумерное соответствие
            является аутлайером (выбросом) при вычислении данной матрицы (то есть это двумерное соответствие некорректно
            для данной матрицы):
            """
            homography_mat, mask_homography = cv2.findHomography(correspondences.points_1, correspondences.points_2,
                                                                 cv2.RANSAC, 4)  # 4 - чисто эмпирическая константа
            essential_mat, mask_essential = cv2.findEssentialMat(correspondences.points_1,
                                                                 correspondences.points_2,
                                                                 intrinsic_mat, cv2.RANSAC, 0.999, 1)
            """
            как мы знаем из лекций, чем хуже два кадра (в нашем случае i и j) связаны гомографией (то есть при
            вычислении матрицы гомографии много аутлайеров среди двумерных соответствий (в mask_homography много нулей))
            и чем лучше на них считается существенная матрица (среди двумерных соответствий много инлайеров при подсчёте
            существенной матрицы - то есть в mask_essential много единиц), тем точнее можно восстановить положения
            камеры на этих кадрах
            (это следует из эпиполярной геометрии, в которой по двум кадрам, переводящимся друг в друга гомографией 
            нельзя восстановить позицию камеры)
            """
            # получаем номера (индексы) двумерных соответствий, которые являются инлайерами для существенной матрицы и
            # аутлайерами для матрицы гомографии -> из того, что только что говорили, следует, что эти двумерные
            # соответствия являются "хорошими" для восстановления позиции камеры:
            inliers_idx = np.arange(len(mask_essential))[(mask_essential.flatten() == 1) & (mask_homography.flatten() == 0)]

            if len(inliers_idx) < 9:  # если таких мало, то восстановить позицию камеры нельзя ->
                continue  # прололжаем искать дальше для других кадров

            # восстанавливаем позицию камеры по "хорошим" двумерным соответствиям, индексы которых у нас
            # лежат в inliers_idx:
            retval, R, t, mask = cv2.recoverPose(essential_mat, correspondences.points_1[inliers_idx],
                                                 correspondences.points_2[inliers_idx], intrinsic_mat)
            """в эту функцию мы подаём существуенную матрицу essential_mat, координаты двумерных соответствий (сначала
            для первого кадра (i) потом для второго (j)) (мы подаём координаты только "хороших соответствий" c индексами
            в inliers_idx) и матрицу внутренних параметров камеры intrinsic_mat; функция восстановит позицию камеры на
            обоих кадрах (точнее - позицию камеры на втором кадре относительно первого) и вернёт retval - количество
            инлайеров среди двумерных соответствий, по которым корректно восстанавливалась позиция камеры, R и t - 
            соответственно матрица поворота и вектор переноса положения камеры на втором кадре относительно положения
            на первом кадре (то есть если камеру, которая сейчас снимает первый кадр (i) повернуть согласно матрице R и 
            сместить на вектор t, то камера окажется в том положении, в котором она снимает второй кадр (j)), mask -
            маска инлайеров"""

            # в качестве метрики (числа), которая будет отражать, насколько успешно по данным кадрам i и j мы смогли
            # восстановить позицию камеры, мы будем использовать отношение числа инлайеров в восстановлении позиции
            # камеры (retval) к общему количеству двумерных соответствий на кадрах i и j (len(correspondences.ids)) -
            # чем больше это число, тем лучше - тем точнее восстановилась позиция камеры по имеющимся двумерным
            # соответствиям.
            # значение этой метрики мы храним в переменной best_inliers.

            # здесь мы запоминаем номера кадров и восстановленную позицию камеры с наибольшм значением best_inliers:
            if best_inliers < retval / len(correspondences.ids):
                best_inliers = retval / len(correspondences.ids)
                best_frames = (i, j)
                best_R_t = (R, t)

    assert (best_R_t is not None)

    first_frame, second_frame = best_frames  # получаем два кадра, на которых нашли положения камеры
    R, t = best_R_t  # положение камеры во втором кадре (относительно первого)
    # помним, что положение камеры в первом кадре принято за точку отсчёта,
    # поэтому его view-матрица = eye3x4() (единичная матрица поворота + нулевой вектор переноса)
    pose_first = view_mat3x4_to_pose(eye3x4())  # положения камеры (в формате Pose()) на обоих кадрах:
    pose_second = Pose(R.T, R.T @ -t)

    print("Инициализация завершена - выбраны кадры: ", first_frame, second_frame)
    print("----------------------------------------------------------------------------------------------------")

    return (first_frame, pose_first), (second_frame, pose_second)  # возвращаем кадры и положения камеры на них


"""---------------------------------------------------------------------------------------------"""
"""==================================== НАСТРАИВАЕМЫЕ ПАРАМЕТРЫ: ==============================="""

REPROJECTION_ERROR = 0.5  # ошибка репроекции при получении 3d-2d соответствий - см лекции
MIN_TRIANGULATION_ANGLE = 2  # минимальный угол триангуляции - см лекции
MIN_DEPTH = 0
PNP_ERROR = 3  # ошибка репроекции при решении задачи pnp
MIN_INLIERS = 20  # минимальное количество инлайеров, которое нас устраивает для решения pnp
MAX_RETRIANGL = 25  # максимальное количество точек, которое ретриангулируем


"""----------------------------------------------------------------------------------------------"""
"""=================================== ОСНОВНАЯ ФУНКЦИЯ ТРЕКИНГА КАМЕРЫ: ========================"""


def track_and_calc_colors(camera_parameters: CameraParameters,  # параметры камеры (фокусное расстаяние + координаты
                          #                                                         центра камеры, кажется...)
                          corner_storage: CornerStorage,  # откуда уголки брать (можно обращаться по индексу =
                          frame_sequence_path: str,                         # = номеру кадра (нумерация с 0))
                          known_view_1: Optional[Tuple[int, Pose]] = None,  # известные положения камеры в двух кадрах
                          #                             ^    ^
                          #                             |    позиция камеры (которые можно перевести во view-матрицу)
                          #                             номер кадра, где задана позиция
                          known_view_2: Optional[Tuple[int, Pose]] = None) \
        -> Tuple[List[Pose], PointCloud]:  # возвращаем словарь позиций камеры (индексирован номерами кадров) и 3d точки

    rgb_sequence = frameseq.read_rgb_f32(frame_sequence_path)
    intrinsic_mat = to_opencv_camera_mat3x3(  # матрица внутренних параметров камеры (где фокусное расст и центр камеры)
        camera_parameters,
        rgb_sequence[0].shape[0]
    )

    # если известные положения камеры в двух кадрах нам не дали на вход функции (то есть они None), то нужно получить
    # их самостотельно (то есть произвести инициализацию - см лекции по курсу), что и делает функция get_initial_frames:
    if known_view_1 is None or known_view_2 is None:
        known_view_1, known_view_2 = get_initial_frames(corner_storage=corner_storage, intrinsic_mat=intrinsic_mat)

    """
    Сразу заводим структуру found_3d_points класса PointCloudBuilder(), в котором будем хранить найденные 3d точки
    (они хранятся вместе с id (то есть номерами) тех уголков, для которых эти 3d точки найдены) 
    
    То есть вспоминаем лекции: у нас есть уголки - это некоторые точки на изображении, которые мы задетектировали; 
    но каждая точка на изображении - это изображение (проекция, фотография) некоторой 3d точки в пространстве 
    (которое мы фотограируем); так вот с помощью специальных методов мы, зная положение и треки уголков на 
    кадрах (это мы получили в corners.py), можем определить для некоторых уголков положение 3d-точек, которые 
    в эти уголки проецируются (то есть уголки с одним и тем же id - это фактически проекции одной и той же 3d точки 
    (и они (эти проекции) могут быть на разных кадрах (например, просто одну и ту же 3d-точку с разных 
    ракурсов фотографируем -> проекция этой (одной и той же) 3d-точки будет на всех этих сделанных кадрах); а 
    отслеживание этого уголка (который являяется проекцией 3d-точки) сквозь эти разные кадры (то есть просто один и тот
    же id на разных кадрах ищем) - и является треком уголка));
    
    Так вот структура found_3d_points хранит найденные нами 3d-точки (просто три координаты тёхмерной точки хранит),
    а также для каждой такой точки в этой структуре будет храниться id того уголка, который является проекцией
    этой 3d-точки (3d-точку мы можем только для уголков получать, поэтому id всегда будет существовать)
    ! эта структура обычно (у нас в лекциях, например) называется облако 3d точек !
    
    P.S. Конечно, мы получаем координаты 3d-точек (как и положения камеры на кадрах) относительно некоторой начальной
    (нулевой точки) - какой именно, нас не интересует, так как нам важно скорее только взаимное положение как 3d-точек,
    так и камеры на кадрах; также все координаты считаются с точностью до масштаба, так как масштаб по одному
    только изображению определить невозможно (то есть все координта мы можем умножить на одно и то же ненулевое число -
    и ничего не поменяется).
    """
    found_3d_points = PointCloudBuilder()  # облако 3d-точек

    """
    А также заводим ещё два массива:
    frame_with_found_cam - номера кадров, где уже нашли положение камеры
    view_mats - эти самые найденные положения камеры    
    """
    frame_with_found_cam = []
    view_mats = []

    """
    И еще один массив заводим, в котором будут храниться индексы (id) тех уголков, которые считаются 
    выбросами (аутлайерами). Таковыми являются уголки, которые были некачественно, неточно или вовсе
    ошибочно задетектированы на изображении (помним, что в corners.py мы используем очень маленькое число для
    качества уголков - это приводит к детектированию большого числа уголков, но на сложных кадрах, в видео, где много
    движения, может быть много ошибочных или плохих уголков (для которых будут потом плохо или вообще неверно вычислены
    треки...); или может быть ситуация, когда уголок изначально хороший и его треки - тоже качественные (соответственно
    для таких кадров мы можем использовать этот уголок для поиска положения камеры), но затем на 
    последующих кадрах что-то произошло (какой-то большой новый предмет появился в кадре или что-то такое) -> и тогда
    наш изначально хороший уголок мог плохо затрекаться в новых кадрах -> он стал выбросом, который для дальнейшего
    поиска положений камеры не стоит использовать). Такие уголки могут ухудшить трекинг камеры, поэтому эти уголки, а 
    точнее - полученные из них 3d-точки, мы будем удалять из found_3d_points. 
    
    Как мы будем получать выбросы? Выбросами мы будем считать те уголки, которые оказались выбросами
    при решении задачи pnp (а выбросы в решении задачи pnp определяются с помощью алгоритма RANSAC)
    """
    IDS_OUTLIERS = np.array([], dtype=np.int64)

    """---------------------------------- ТРИАНГУЛЯЦИЯ ----------------------------------------------------"""
    """
    Для начала - ТРИАНГУЛЯЦИЯ, то есть ищем трехмерные (3d) точки, которые соответствуют 2d-точкам (точнее - 
    уголкам) на двух кадрах с уже известными позициями камеры
    
    Чтобы произвести триангуляцю, нам сначала нужно получить двумерные соответствия на имеющихся двух
    кадрах (для которых уже есть положения камеры) - то есть найти уголки, которые встречаются на обоих кадрах 
    (так как у нас уголок обладает своим уникальным id, то для этого просто ищем уголки с одинаковым id на обоих
    кадрах) - эти уголки являются изображениями некоторых 3d точек (которые на этих двух кадрах просто с разных ракурсов
    снимали) -> и вот задача триангуляции заключается в том, чтобы получить координаты этих 3d точек.
    (то есть для двух кадров двумерным соответствием является пара проекций (одно на одном кадре, другое на другом)
    одной и той же 3d-точки)
    
    Так как уголки подсчитаны неидеально, то не для каждого уголка, участвующего в двумерных соответствиях, можно
    хорошо найти 3d-точку. Но так как это первичная триангуляция, то хотя бы три 3d-точки мы получить должны (иначе
    дальше не получится восстановить положения камеры) - для этого устанавливаем мягкие параметры триангуляции:
    делаем большую ошибку репроекции (ошибка репроекции - это максимально расстояние в пикселях от проекции 
    (на один из кадров) вычисленных 3d точек до исходных проекций (- уголков), по которым эти 3d точки считали; чем
    больше ошибка репроекции, тем менее точно мы позволяем считать 3d-точки - но в первичной триангуляции мы хоть как-то
    должны их посчитать, поэтому берём большую ошибку) и никак не ограничиваем угол триангуляции и глубину...
    """
    print("Начало исходной триангуляции...")
    known_frame_1 = known_view_1[0]  # номер первого кадра, для которого известно положение камеры
    known_frame_2 = known_view_2[0]  # номер второго кадра
    known_view3x4_1 = pose_to_view_mat3x4(known_view_1[1])  # известные view-матрицы позиций камеры в обоих кадрах
    known_view3x4_2 = pose_to_view_mat3x4(known_view_2[1])  # (по-другому называются: матрицы внешних мараметров камеры)

    if known_frame_1 > known_frame_2:  # делаем так, чтобы второй изестный кадр по номеру был больше первого
        known_frame_1, known_frame_2 = known_frame_2, known_frame_1
        known_view3x4_1, known_view3x4_2 = known_view3x4_2, known_view3x4_1

    frame_with_found_cam.append(known_frame_1)  # добавляем то, что известно в массивы
    frame_with_found_cam.append(known_frame_2)
    view_mats.append(known_view3x4_1)
    view_mats.append(known_view3x4_2)

    known_1_corners = corner_storage[known_frame_1]  # уголки в кадрах, для которыз позиции камеры известны
    known_2_corners = corner_storage[known_frame_2]            # (имеют тип FrameCorners)

    triang_params = TriangulationParameters(max_reprojection_error=5,  # параметры для первичной триангуляции задаем
                                            min_triangulation_angle_deg=0,  # самые мягкие, потому что уж первчиная
                                            min_depth=0)  # триангуляция точно должна пройти успешно

    correspondences_known_1_2 = build_correspondences(known_1_corners, known_2_corners)  # строим пересечение уголков
    # на двух кадрах - те ищем одни и те же уголки (у них один и тот же id) на двух кадрах - это наши двумерные
    # соответствия - далее по этим двумерным соответствиям восстанавливаем 3d точки, проецирующиеся в эти уголки -
    # это и есть процедура триангуляции
    assert (len(correspondences_known_1_2.ids) > 3)  # проверяем, что у нас достаточно много двумерных соответствий
    # (иначе триангуляция просто невозможна)

    points3d_for_1and2, ids_common_known_corners, median_cos = \
        triangulate_correspondences(correspondences=correspondences_known_1_2,
                                    view_mat_1=known_view3x4_1, view_mat_2=known_view3x4_2,
                                    intrinsic_mat=intrinsic_mat, parameters=triang_params)  # по двумерным соответсвтиям
    # восстанавливаем 3d точки (points3d_for_1and2), а также получаем индексы (то есть id) (ids_common_known_corners)
    # для этих точек (для каждой точки из points3d_for_1and2 есть соответствующее id в ids_common_known_corners), эти id
    # - это id уголков, в которые эти 3d-точки проецируются (то есть для уголков с этими id мы восстановили 3d-точки,
    # изображениями которых эти уголки являются)
    assert (len(ids_common_known_corners) > 3)  # проверяем, что у нас достаточно много 3d точек

    found_3d_points.add_points(ids=ids_common_known_corners, points=points3d_for_1and2)  # добавляем точки в облако

    print("           ... исходная триангуляция завершена")
    print()
    """---------------------------- закончили триангуляцию -------------------------------------------------"""

    """--------------начинаем основной цикл решения 2d-3d соответствий-------------------------------------------"""

    """
    Итак, на данный момент у нас известны положения камеры в кадрах с номерами known_frame_1 и known_frame_2 (эти 
    положения нам либо дали на вход, либо мы их нашли с помощью процедуры инициализации, которую выполняет функция
    get_initial_frames), а также у нас уже есть облако 3d-точек found_3d_points - в нём пока немного точек, так как
    всего лишь два положения камеры известно, но сколько есть...
    
    Таким образом, на данный момент всё множество кадров (с номерами от 0 до len(corner_storage) - 1 включительно), для
    которых мы ещё НЕ знаем позицию камеры, поделилось на три части (границы частей (=номера кадров) - включительно):
    1) [0 ... known_frame_1 - 1]
    2) [known_frame_1 + 1 ... known_frame_2 - 1]
    3) [known_frame_2 + 1 ... len(corner_storage) - 1]
    (конечно, какие-то части могут быть вырожденными (то есть содержать ноль кадров), если, например, 
    известные кадры - это первые два кадра (то есть known_frame_1 = 0 и known_frame_2 = 1 -> тогда 1) и 2) части
    пустые (не содержат кадров), а все неизвестные кадры собраны в 3) части))
    
    И далее нам нужно быдет выбирать кадр из одной из этих трёх частей, чтобы далее решать задачу PNP (задача 
    Perspective-n-Point (сокращённо - pnp) заключается в том, чтобы по известным 3d-точкам, а также по 
    2d-точкам (уголкам) на изображении восстановить положение камеры в этом изображении -> для этого находятся 2d-3d
    соответствия - то есть среди имеющихся 3d-точек находятся те, чьи проекции являются 2d-точками (уголками) на 
    изображении - эти 2d-точки являются соответствующими этим 3d-точками (уже говорилось, что характеристикой как 
    уголков, так и 3d-точек является их id: уголки с одним и тем же id - это просто проекции конкретной 3d-точки 
    (которой приписан тот же id); поэтому для нахождения 2d-3d соответствий, достаточно просто найти одни и те же 
    id среди 3d-точек в облаке и среди уголков на изображении) -> а далее по этим соответствиям (в общем случае 
    их n штук (то есть n пар из соответствующих 3d и 2d точек) - поэтому и pnp) решаются уравнения и находится положение
    камеры в этом кадре... подробнее - см лекции) с точками этого кадра.
    Но выбирать мы будем лучший подходящий кадр - то есть тот, для которого наибольшее количество соответстующих
    2d и 3d точек (то есть больше 2d-3d соответствий... - чем их больше, тем задача pnp даёт более точный результат).
    
    Делать будем так. Изначально нас есть две области номеров кадров, для которых уже знаем положения камеры - каждая из 
    этих областей изначально состоит из одного кадра: known_frame_1 и known_frame_2 соответственно; эти области и делят 
    все наши кадры на три части 1), 2), 3), что уже сказали в начале этого комментария. То есть выглядит это так:
    
                                         [...]        [...]             - две области с известными положениями камер
                                  [....]       [....]       [....]      - три части с неизвестными положениями камер
    эти две области делят всё множество кадров на эти три части - а все вместе эти области и части дают:
                                 [................................]     - всё множество кадров
                                 
    Тепрь из одной из трёх частей (с кадрами с неизвестными положениями камер) мы хотим выбрать один кадр, у которого
    наибольшее количество 2d-3d соответствий для задачи pnp. Так как 2d-3d соответствия получаются из известных
    3d-точек и уголков на самом изображении, то нам нужно найти кадр, у которого как можно больше пересечений уголков
    с известными 3d-точками. Но известные 3d-точки (которые образуют облако 3d-точек) получаются триангуляцией из 
    уголков на кадрах с уже известными положениями камеры, поэтому самым логичнысм будет искать кадр, у которого
    наибольшее пересечение уголков с уголками на одном из кадров, для которого уже известно положение камеры - 
    те кадров одной из двух имеющихся областей. А так как уголки на кадрах мы получаем методом оптического потока (в
    corners.py мы трекаем уголки с помощью алгоритма Лукаса-Канаде - это и есть оптически поток) - то есть двигаемся от
    кадра к кадру по порядку и отслеживаем уголки, то кадром, у которого наибольшее пересечение по уголкам с кадром из
    одной из двух областей может быть только кадр СОСЕДНИЙ с этими двумя областями (так как метод оптического потока
    в этом и заключается, что он отслеживат еголки от кадра к следующем кадру по порядку -> чем ближе кадры, тем больше
    у них общих уголков).
    Таким образом, в качестве следующего кадра, для которого мы будем решать задачу pnp мы берём один из кадров,
    соседних с имеющимися двумя областями кадров с известными положениями камер. Далее мы решаем для этого кадра
    задачу pnp -> находим положение камеры в этом кадре - то есть этот кадр присоединяется к областям с известными
    положениями камер -> делаем триангуляцию, дополняя облако 3d-точек -> далее нам снова нужно выбрать соседний с 
    областями кадр и снова решать pnp и тд....... То есть каждый раз мы берем кадр, соседний с одной из областью,
    находим на этом кадре положение камеры, а значит наша область с известными положениями камеры расширяется (заметим,
    что так как кадр граничил с одной из двух областей, то расширение области заключается просто в сдвиге границы
    одной из двух областей на новый кадр -> те областей с известными положениями камеры так и остаётся две штуки).
    
    Таким образом, в течении всей работы программы у нас будет две области кадров с известными положениями камеры:
    1) [left_lim_1, right_lim_1]  - первая область (left_lim_1, right_lim_1 - номера кадров - границы этой области)
    2) [left_lim_2, right_lim_2]  - вторая область
    и каждый раз мы будем выбирать для решения pnp один из четырёх соседних с этими областями кадров: left_lim_1 - 1,
    right_lim_1 + 1, left_lim_2 - 1, right_lim_2 + 1   (конечно, в какой-то момент области соприкоснутся (left_lim_2 =
    = right_lim_1 - 1) и дойдут до одной из границ множества всех кадров (left_lim_1 = 0 или rith_lim_2 = 
    = len(corner_storage) - 1) - тогда кадров для выбора станет меньше 4...)
    
    Итак, будем поддерживать гарницы этих двух областей. Изначально каждая область состоит из одного кадра known_frame_1
    и known_frame_2 -> границы такие:
    """
    left_lim_1 = known_frame_1  # границы первой области
    right_lim_1 = known_frame_1
    left_lim_2 = known_frame_2  # границы второй области
    right_lim_2 = known_frame_2

    num_iter = 0  # подсчитываем число итераций главного цикла
    while True:  # глвный цикл, в котором происходит получение положений камеры
        if left_lim_1 == 0 and right_lim_2 == len(corner_storage) - 1 \
                and right_lim_1 == left_lim_2 - 1:  # наши две области кадров с известными положениями
            # камеры покрыли все кадры -> можно выходить из цикла

            assert (len(view_mats) == len(corner_storage))  # заодно проверям, что нашли view-матрицы для всех кадров
            break

        print("Шаг номер: ", num_iter, ",")

        new_frame, left_lim_1, right_lim_1, left_lim_2, right_lim_2, rvec, tvec, inliers, curr_outliers = \
            choose_best_next_frame_and_solve_pnp(left_lim_1, right_lim_1, left_lim_2, right_lim_2,
                                                 found_3d_points.points, found_3d_points.ids,
                                                 intrinsic_mat, corner_storage,
                                                 PNP_ERROR, MIN_INLIERS,
                                                 frame_with_found_cam, view_mats)  # ищем лучший кадр, считаем
        # для него положения камеры (в веде rvec, tvec - параметры Родригеса) и заодно двигаем границу областей
        # кадров, для которых нашли положения камер -> в результате нашли положение камеры для нового кадра

        assert (left_lim_1 <= right_lim_1 < left_lim_2 <= right_lim_2)  # проверяем корректность наших границ

        IDS_OUTLIERS = np.append(IDS_OUTLIERS, curr_outliers)  # добавляем индексы аутлайеров (- тех точек, на которых
        # не решалась задача pnp - то есть они возможно выбросы)
        IDS_OUTLIERS.sort()  # обязательно сортируем, так как далее мы используем этот массив в build_correspondences,
        # а внутри этой функции есть snp.intersect, который работает правильно только с отсортированными массивами!
        found_3d_points.delete_points(IDS_OUTLIERS)  # удаляем аутлайеры из 3d-точек, чтобы они не мешали дальше

        print("Кадр ", new_frame, " обработан; количество инлайеров, по которым решена pnp = ", len(inliers), ",")
        print("Текущиие кадры, для которых нашли положение камеры: [", left_lim_1, " ... ",
              right_lim_1, "], [", left_lim_2, " ... ", right_lim_2, "], а всего кадров: ", len(corner_storage), ".")
        print("-------------------------------------------------------")
        print()

        # у нас появился новый кадр new_frame, для которого знаем положение камеры -> можем дополнть облако 3d-точек:

        """-----------теперь попытаемся дотриангулировать еще 3d точек-------------------"""
        new_view_camera = rodrigues_and_translation_to_view_mat3x4(rvec, tvec)  # получили view матрицу для нового кадра
        new_corners = corner_storage[new_frame]  # и уголки на нём

        for prev_frame, prev_view_camera in zip(frame_with_found_cam[::3], view_mats[::3]):  # перебираем все
            # предыдущие кадры prev_frame, для которых уже извстна view-матрица prev_view_camera положения камеры
            # (и сами матрицы тоже перебираем)  -- но так как прямо все кадры перебирать долго, перебираем, например,
            # с шагом 3
            prev_corners = corner_storage[prev_frame]  # уголки в предыдущем кадре

            triang_params = TriangulationParameters(max_reprojection_error=REPROJECTION_ERROR,
                                                    min_triangulation_angle_deg=MIN_TRIANGULATION_ANGLE,
                                                    min_depth=MIN_DEPTH)  # задаем параметры триангуляции
            correspondences_prev_new = build_correspondences(prev_corners, new_corners,
                                                             ids_to_remove=IDS_OUTLIERS)  # снова находим общие уголки
            # на двух кадраха - то есть 2d-соответствия на этих кадрах

            if len(correspondences_prev_new.ids) < 4: continue  # если их слишком мало - переходим к следующему кадру
            new_3d_points, prev_new_common_ids, median_cos = \
                triangulate_correspondences(correspondences=correspondences_prev_new,
                                            view_mat_1=prev_view_camera, view_mat_2=new_view_camera,
                                            intrinsic_mat=intrinsic_mat, parameters=triang_params)  # триангуляция:
            # получаем 3d-точки и индексы (id) уголков, соответствующих этим 3d-точкам

            if len(prev_new_common_ids) > 0:  # если нашли больше нуля точек:
                # добавляем новые 3d точки в облако - но только те, которых ещё нет:
                # (то, что добавятся только новые 3d точки, гарантируется самим методо add_points - он добавляет только
                # еще несуществующие 3d-точки):
                found_3d_points.add_points(ids=prev_new_common_ids, points=new_3d_points)

        frame_with_found_cam.append(new_frame)  # добавляем новый кадр с известной камерой в этот массив
        view_mats.append(new_view_camera)  # добавляем соответсвующее положение (view-матрицу) камеры

        """----------а теперь - ретириангуляция, то есть триангулируем точки по нескольким кадрам-------------"""
        if num_iter % 10 == 0 and num_iter > 10:  # ретриангулируем каждые 10 кадров (раз в 10 кадров)
            count_retriang = 0
            for known_3d_point in found_3d_points.ids[-MAX_RETRIANGL:]:  # ретириангулируем только последние
                #                                                          MAX_RETRIANGL точек
                if count_retriang == MAX_RETRIANGL: break
                points2d_for_this_3d_point = []  # 2d-точки, соответствующие взятой 3d-точке
                view_mats_for_frames_with_this_3d_point = []  # view-матрицы для кадров, в которых эта 3d-точка видна
                for frame, view_m in zip(frame_with_found_cam, view_mats):  # перебираем кадры с известными view-матрицами
                    if known_3d_point in corner_storage[frame].ids:  # если 3d-точка есть на кадре
                        index_3d_point = list(corner_storage[frame].ids).index(known_3d_point)
                        points2d_for_this_3d_point.append(corner_storage[frame].points[index_3d_point])
                        view_mats_for_frames_with_this_3d_point.append(view_m)

                if len(view_mats_for_frames_with_this_3d_point) < 3:  # если маловато кадров - идем дальше
                    continue
                else:
                    new3d = retriangulate_points(proj_mat=intrinsic_mat,
                                                 view_mat_sequence=view_mats_for_frames_with_this_3d_point,
                                                 points2d_sequence=points2d_for_this_3d_point)  # получаем 3d-точку -
                    # - это ретриангулированная точка known_3d_point (её положение было уточнено с помощью view-матриц
                    # из массива view_mats_for_frames_with_this_3d_point - те по нескольким кадрам уточнили точку)
                    found_3d_points.update_points(ids=np.array([known_3d_point]), points=new3d.reshape(1, 3))  # обновляем
                count_retriang += 1
        num_iter += 1

    """
    На данный момент можно считать, что трекинг камеры завершён: мы получили обако 3d-точек found_3d_points и
    положения камеры (в виде view-матриц) во всех кадрах - они содержатся в массиве view_mats.
    
    Далее несколько строк преобразуют полученные нами данные в подходящий вид, какой требуется внутри проекта...
    """
    point_cloud_builder = found_3d_points
    calc_point_cloud_colors(
        point_cloud_builder,
        rgb_sequence,
        view_mats,
        intrinsic_mat,
        corner_storage,
        5.0
    )
    point_cloud = point_cloud_builder.build_point_cloud()  # получаем итоговое облако точек после некоторых манипуляций,
    # необходимых для отображения этих точек... (нужно в других файлах проекта)

    """
    ОЧЕНЬ ВАЖНО! 
    во view_mats мы добавляли матрицы камер в том порядке, в котором мы их находили (а могли найти сначла камеру
    для 1 кадра, затем для десятого...), а вернуть нам нужно матрицы в том же порядке, в котором у нас идут кадры - 
    те сначала должна идти view-матрица (которую мы в pose переделываем) для 0-го кадра, затем для 1-ого и тд
    
    Так как параллельно с добавлением матриц во view-mats мы еще и добавляли номера кадров, для которых нашли
    эту view-матрицу, то просто отсортируем view-mats по frame_with_found_cam, в котором эти номера кадров и хранятся
    (то есть переставим элементы view_mats так, чтобы соответствующие номера кадров во frame_with_found_cam встали
    по порядку) - это и делаем далее:
    
    (этой ошибки я долго не замечал -> из-за этого все время были большие ошибки, так как выводимые pose камер не 
    соответствовали кадру)
    """
    temp = sorted(zip(frame_with_found_cam, view_mats), key=lambda x: x[0])
    frame_with_found_cam = [vm[0] for vm in temp]
    view_mats = [vm[1] for vm in temp]  # view-матрицы камеры, идущие в том же порядке, что и кадры

    poses = list(map(view_mat3x4_to_pose, view_mats))  # получаем позиции камеры в каждом кадре
    return poses, point_cloud  # наконец, возвращаем позиции камеры и облако точек


if __name__ == '__main__':
    # pylint:disable=no-value-for-parameter
    create_cli(track_and_calc_colors)()
